PoC: Agente de Chat RAG com Ollama e LlamaIndex
Este √© um prot√≥tipo (Proof of Concept) de um agente de chat local que utiliza a t√©cnica RAG (Retrieval-Augmented Generation). O objetivo √© permitir que um usu√°rio fa√ßa perguntas em linguagem natural e receba respostas baseadas apenas em um conjunto de documentos locais (PDFs, TXTs, etc.).

Este projeto usa LlamaIndex para orquestrar o RAG, Ollama para rodar os modelos de IA localmente, e Flask para servir como um backend de API simples.

üöÄ Como Funciona
A arquitetura √© dividida em quatro componentes principais:

Frontend (chatfullscreen.html): Uma interface de chat simples onde o usu√°rio envia perguntas.

Backend (app.py): Um servidor Flask que "ouve" o frontend. Ele recebe a pergunta do usu√°rio.

RAG (LlamaIndex): O "c√©rebro" do backend. Quando recebe uma pergunta:

Ele usa o modelo de embedding (nomic-embed-text) para "entender" o significado da pergunta.

Ele busca nos documentos da pasta dados_intelbras pelos trechos mais relevantes.

Ele monta um novo prompt para o LLM, contendo a pergunta do usu√°rio e os trechos de contexto encontrados.

IA Local (Ollama): O "motor" de IA.

Ele serve o modelo de embedding (nomic-embed-text).

Ele recebe o prompt do LlamaIndex e usa o LLM principal (phi-3) para gerar a resposta.

üìã Requisitos
Antes de come√ßar, garanta que voc√™ tem os seguintes requisitos:

Python: Python 3.11 (vers√µes 3.12 ou 3.13 ir√£o falhar com erros de depend√™ncia).

Ollama: O aplicativo Ollama instalado e rodando em segundo plano.

RAM: Pelo menos 16 GB de RAM (o modelo Phi-3 √© leve, mas o processo de RAG consome mem√≥ria).

Arquivos de Dados: Uma pasta chamada dados_intelbras no mesmo diret√≥rio do app.py, contendo seus arquivos de conhecimento (ex: manuais em PDF da Intelbras).

üõ†Ô∏è Configura√ß√£o do Ambiente
Siga estes passos exatamente para configurar seu ambiente de desenvolvimento.

1. Instalar Python 3.11
Se voc√™ n√£o tem o Python 3.11, baixe o instalador do site oficial do Python (Windows 64-bit).

Importante: Durante a instala√ß√£o, marque a caixa "Add python.exe to PATH".

2. Baixar os Modelos Ollama
Com o Ollama j√° instalado e rodando, abra um terminal (PowerShell) e puxe os dois modelos que o script app.py utiliza:

PowerShell

# 1. O LLM Principal (o "C√©rebro")
ollama pull phi-3:3.8b-mini-128k-instruct-q4_K_M

# 2. O Modelo de Embedding (o "Catalogador")
ollama pull nomic-embed-text
3. Criar o Ambiente Virtual Python
Este passo √© crucial para isolar as depend√™ncias.

PowerShell

# 1. Navegue at√© a pasta do seu projeto (ex: hackaton-poc)
cd C:\Caminho\Para\Seu\Projeto

# 2. Crie um ambiente virtual usando Python 3.11
py -3.11 -m venv .venv

# 3. Permita a execu√ß√£o de scripts no PowerShell (apenas para esta janela)
Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass

# 4. Ative o ambiente virtual
.\.venv\Scripts\Activate.ps1

# O seu prompt deve mudar para: (.venv) PS C:\...
4. Instalar as Depend√™ncias
Com o ambiente (.venv) ativo, instale todas as bibliotecas Python de uma vez. O pip cache purge garante que voc√™ n√£o use pacotes quebrados de um cache antigo.

PowerShell

# Limpa o cache do pip (recomendado)
python -m pip cache purge

# Instala todas as bibliotecas necess√°rias
python -m pip install --no-cache-dir --upgrade llama-index llama-index-llms-ollama llama-index-embeddings-ollama flask flask-cors pypdf
üèÉ Como Executar
Ap√≥s a configura√ß√£o, siga estes passos para rodar o projeto:

Garanta que o Ollama est√° rodando em segundo plano (verifique o √≠cone na bandeja do sistema).

Abra seu terminal (PowerShell) na pasta do projeto.

Ative o ambiente virtual (se ainda n√£o estiver ativo):

PowerShell

.\.venv\Scripts\Activate.ps1
Execute o servidor de backend:

PowerShell

python app.py

O terminal mostrar√° os logs. Espere pela mensagem: INFO:root:Query Engine est√° pronto. Servidor pronto para receber perguntas.

Abra o arquivo chatfullscreen.html no seu navegador (clique duplo no arquivo).

Agora voc√™ pode fazer perguntas no chat.

Para parar o servidor de backend, volte ao terminal e pressione Ctrl + C.

‚ö†Ô∏è Solu√ß√£o de Problemas (Troubleshooting)
ERRO: model requires more system memory...

Causa: O Ollama est√° "preso" tentando carregar um modelo antigo ou "fantasma" que exige mais RAM do que voc√™ tem.

Solu√ß√£o:

Pare o servidor (Ctrl + C).

Reinicie o aplicativo Ollama (clique com o bot√£o direito no √≠cone da bandeja do sistema > "Quit").

Se isso n√£o funcionar, abra o Gerenciador de Tarefas do Windows (Ctrl+Shift+Esc), v√° para "Detalhes", encontre ollama.exe e clique em "Finalizar tarefa".

Reinicie o Ollama e rode o python app.py novamente.

ERRO: model ... not found (status code: 404)

Causa: O nome do modelo no app.py (linha 36) n√£o √© exatamente igual ao nome do modelo na sua lista do Ollama.

Solu√ß√£o:

Rode ollama list no terminal.

Copie o nome exato do modelo (ex: phi-3:3.8b-mini-128k-instruct-q4_K_M).

Cole esse nome no app.py na linha Settings.llm = Ollama(model="...").

ERRO: ModuleNotFoundError ou ImportError

Causa: Voc√™ n√£o ativou o ambiente virtual antes de rodar o python app.py.

Solu√ß√£o: Pare o script (Ctrl + C), rode .\.venv\Scripts\Activate.ps1 e tente novamente.